{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-02T05:42:27.617665Z",
     "start_time": "2025-02-02T05:41:03.687961Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from  transformers import  pipeline\n",
    "\n",
    "http_proxy = os.getenv(\"http_proxy\")\n",
    "\n",
    "print(http_proxy)\n",
    "\n",
    "pipe = pipeline(\"sentiment-analysis\")\n",
    "# pipe(\"今儿上海可真冷啊\")\n",
    "\n",
    "text_list =[\n",
    "    \"今儿上海可真冷啊\",\n",
    "    \"哈哈\"\n",
    "]\n",
    "\n",
    "pipe(text_list)\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://172.30.96.1:10809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.8957215547561646},\n",
       " {'label': 'NEGATIVE', 'score': 0.7850582599639893}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T06:42:58.414601Z",
     "start_time": "2025-02-02T06:42:49.545600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipe = pipeline(task=\"ner\",model=\"ckiplab/bert-base-chinese-ws\")\n",
    "\n",
    "preds = ner_pipe(\"你好世界\")\n",
    "\n",
    "preds = [\n",
    "    {\n",
    "        \"entity\": pred[\"entity\"],\n",
    "        \"score\": round(pred[\"score\"], 4),\n",
    "        \"index\": pred[\"index\"],\n",
    "        \"word\": pred[\"word\"],\n",
    "        \"start\": pred[\"start\"],\n",
    "        \"end\": pred[\"end\"],\n",
    "    }\n",
    "    for pred in preds\n",
    "]\n",
    "print(*preds, sep=\"\\n\")\n",
    "\n",
    "\n"
   ],
   "id": "c9ab72699992cf0e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'B', 'score': 1.0, 'index': 1, 'word': '你', 'start': 0, 'end': 1}\n",
      "{'entity': 'B', 'score': 0.9958, 'index': 2, 'word': '好', 'start': 1, 'end': 2}\n",
      "{'entity': 'B', 'score': 0.992, 'index': 3, 'word': '世', 'start': 2, 'end': 3}\n",
      "{'entity': 'I', 'score': 0.9999, 'index': 4, 'word': '界', 'start': 3, 'end': 4}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 一些常用命令\n",
    "conda 命令\n",
    "\n",
    "```shell\n",
    "// 查看当前有那些环境\n",
    "conda info --envs\n",
    "// 激活环境\n",
    "conda activate myenv\n",
    "```\n",
    "\n",
    "win 中使用wsl 安装linux\n",
    "\n",
    "安装 OpenSSH 服务器\n",
    "```\n",
    "sudo apt update\n",
    "sudo apt install openssh-server\n",
    "-- 启动服务\n",
    "sudo service ssh start\n",
    "\n",
    "-- 查看ssh 服务是否启动\n",
    "systemctl status ssh\n",
    "```\n",
    "\n",
    "需要使用ip，不能使用localhost\n",
    "\n",
    "\n",
    "配置系统代理\n",
    "```\n",
    "export http_proxy=\"http://localhost:7890\"\n",
    "export https_proxy=\"http://localhost:7890\"\n",
    "```\n",
    "\n",
    "为conda 某个环境配置配置环境变量\n",
    "```\n",
    "-- 创建该文件\n",
    "touch $CONDA_PREFIX/etc/conda/activate.d/proxy_setup.sh\n",
    "-- 写入以下内容\n",
    "#!/bin/bash\n",
    "export http_proxy=\"http://proxy.example.com:8080\"\n",
    "export https_proxy=\"http://proxy.example.com:8080\"\n",
    "-- 设置权限\n",
    "chmod +x $CONDA_PREFIX/etc/conda/activate.d/proxy_setup.sh\n",
    "```\n",
    "\n",
    "\n",
    "设置hugging face 下载的模型缓存目录\n",
    "\n",
    "```\n",
    "export TRANSFORMERS_CACHE=\"/new/path/to/model/cache\"\n",
    "-- 停用当前激活的 Conda 环境\n",
    "conda deactivate\n",
    "-- 重新激活目标 Conda 环境\n",
    "conda activate your_env_name\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "设置代理，能够访问 hugging face\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f8f160dbffb4d797"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T08:38:24.778659Z",
     "start_time": "2025-02-02T08:38:14.315319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "## 问答模型\n",
    "qa  = pipeline(task= \"question-answering\" ,model= \"impira/layoutlm-document-qa\")\n",
    "\n",
    "\n",
    "preds = qa( question=\"中国的首都在哪里？\",context=\"我不知道\")\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"score: {preds['score']}, start: {preds['start']}, end: {preds['end']}, answer: {preds['answer']}\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "8d93d4c3ba31b14a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'LayoutLMForQuestionAnswering' is not supported for question-answering. Supported models are ['AlbertForQuestionAnswering', 'BartForQuestionAnswering', 'BertForQuestionAnswering', 'BigBirdForQuestionAnswering', 'BigBirdPegasusForQuestionAnswering', 'BloomForQuestionAnswering', 'CamembertForQuestionAnswering', 'CanineForQuestionAnswering', 'ConvBertForQuestionAnswering', 'Data2VecTextForQuestionAnswering', 'DebertaForQuestionAnswering', 'DebertaV2ForQuestionAnswering', 'DiffLlamaForQuestionAnswering', 'DistilBertForQuestionAnswering', 'ElectraForQuestionAnswering', 'ErnieForQuestionAnswering', 'ErnieMForQuestionAnswering', 'FalconForQuestionAnswering', 'FlaubertForQuestionAnsweringSimple', 'FNetForQuestionAnswering', 'FunnelForQuestionAnswering', 'GPT2ForQuestionAnswering', 'GPTNeoForQuestionAnswering', 'GPTNeoXForQuestionAnswering', 'GPTJForQuestionAnswering', 'IBertForQuestionAnswering', 'LayoutLMv2ForQuestionAnswering', 'LayoutLMv3ForQuestionAnswering', 'LEDForQuestionAnswering', 'LiltForQuestionAnswering', 'LlamaForQuestionAnswering', 'LongformerForQuestionAnswering', 'LukeForQuestionAnswering', 'LxmertForQuestionAnswering', 'MarkupLMForQuestionAnswering', 'MBartForQuestionAnswering', 'MegaForQuestionAnswering', 'MegatronBertForQuestionAnswering', 'MistralForQuestionAnswering', 'MixtralForQuestionAnswering', 'MobileBertForQuestionAnswering', 'MPNetForQuestionAnswering', 'MptForQuestionAnswering', 'MraForQuestionAnswering', 'MT5ForQuestionAnswering', 'MvpForQuestionAnswering', 'NemotronForQuestionAnswering', 'NezhaForQuestionAnswering', 'NystromformerForQuestionAnswering', 'OPTForQuestionAnswering', 'QDQBertForQuestionAnswering', 'Qwen2ForQuestionAnswering', 'Qwen2MoeForQuestionAnswering', 'ReformerForQuestionAnswering', 'RemBertForQuestionAnswering', 'RobertaForQuestionAnswering', 'RobertaPreLayerNormForQuestionAnswering', 'RoCBertForQuestionAnswering', 'RoFormerForQuestionAnswering', 'SplinterForQuestionAnswering', 'SqueezeBertForQuestionAnswering', 'T5ForQuestionAnswering', 'UMT5ForQuestionAnswering', 'XLMForQuestionAnsweringSimple', 'XLMRobertaForQuestionAnswering', 'XLMRobertaXLForQuestionAnswering', 'XLNetForQuestionAnsweringSimple', 'XmodForQuestionAnswering', 'YosoForQuestionAnswering'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 3.374588386506616e-10, start: 0, end: 4, answer: 我不知道\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T02:09:18.284591Z",
     "start_time": "2025-02-03T02:09:11.686939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "dataset[\"train\"][1111]"
   ],
   "id": "ee74fb4d7f365a84",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': \"been here a number of times and it's my favorite German food in Pgh!\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T02:19:21.277867Z",
     "start_time": "2025-02-03T02:19:21.255454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    print(df)\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "\n",
    "show_random_elements(dataset[\"train\"])\n",
    "\n",
    "print(dataset[\"train\"].features.items())\n",
    "\n",
    "print(datasets.ClassLabel)"
   ],
   "id": "5515d30361db26be",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                               text\n",
      "0      2  Traditional \\\"manly\\\" place with wood walls, f...\n",
      "1      1  Things have changed tremendously at Tamba from...\n",
      "2      1  Yasu is usually one of the best sushi restaura...\n",
      "3      1  My husband and I ordered a couple of recliners...\n",
      "4      3  Great customer service and The staff is really...\n",
      "5      2  JUST OK. Sometimes its hard to explain exactly...\n",
      "6      2  I have been to Liberty a few times now. And I'...\n",
      "7      1  Went with the family on a Saturday night.  Cas...\n",
      "8      4  I am very picky with my Greek lemon soup. For ...\n",
      "9      3  We love sushi and we have been to some of the ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Traditional \\\"manly\\\" place with wood walls, furniture, and older gentlemen as the barbers.  Haircut includes back of neck and sideburn shave, hot towel over the face, and brief shoulder/neck massage with those old school electric massage things.\\n\\n$19 for a nice clean cut is certainly worth it.\\n\\nLocated NE corner of Cactus &amp; Tatum, over by Radio Shack.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Things have changed tremendously at Tamba from my earlier visit, which was about 2 years back. Unfortunately, things have taken a downturn.\\n\\nParking is a pain. If you can walk or take a taxi, do that instead. \\n\\nFirst thing that throws us off is the prepay for lunch buffet. I mean, this is the only restaurant I know where you have to prepay for the buffet. Yes, the spread of the dishes is good; that is where we were deceived :(\\n\\nThe veggie dishes were bland and lacked the flavor and punch of the Indian spices! The 'Malai Kofta' was too salty. The 'saag paneer' lacked paneer and was only a mishmash of spinach; the chole was OK and lacked the punch as well. The Idlis: like rubber! Upma was OK. \\n\\nFrom the desserts, the Gulab Jamun was OK and the Custard was alright too. But then, he placed small containers to fill them up. We filled our stomachs with mostly the desserts(not a healthy way, I know) and for a high-price of almost $13 per person. \\n\\nThere are better Indian restaurants, I would recommend. Go to Gandhi's or India Place instead and be treated to some flavorful food and better service.\\n\\nAnd waiters in Tamba: Grow up and dont play 'Catch ball' when you have customers...and by 'Ball' I mean...you know what?\\n\\nAnd to end this, I AM NOT GOING THERE AGAIN!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Yasu is usually one of the best sushi restaurants I have ever had the pleasure of being in. It consistently wows, with inventive dishes and a definitive style that always have me wishing I was walking back in when I'm finally heading out. Yasu himself is charming and helpful and they have one wonderful, awesome waiter- an American with a suspiciously Japanese name who goes out of his way to help. \\n\\nWe did not get him. \\n\\nOn a Sunday at 7pm myself and 5 friends met for our reservation. We are all repeat customers. HEAVY repeat customers. \\n\\nWe ordered drinks and about the time they finally showed up, a cool 25 minutes later, we were informed they were all out of tuna. \\n\\nWrr? We all sighed and said... ok and ticked off an awful lot of other sushi options which we then turned in.  A few minutes later, our waitress (emphasis on the 'wait') returned and said that the wait was 35 minutes for sushi and would we care to order from the kitchen? We reluctantly said ok and placed a few orders. \\n\\nSo imagine our surprise when moments later our sushi began to arrive. Well. Sort of. Our waitress appeared with 3 uni shots instead of 6. All 6 of us eyed them, wondering who we'd have to take out to get one, and I calmly ask the waitress to retrieve our other uni shots \\\"expedited, perhaps\\\"?  Instead our waitress walked around the restaurant puzzedly WITH the uni shots like a nomad until she returned 5 minutes later to say there was no more uni.  A nice bunch, we all surrendered our shots until the three smartest amongst us grabbed them and then crowed about their taste.  The waitress offered us 3 oyster shots instead. They sucked. \\n\\nSo it was odd when oysters showed up WITH uni on them (though we should say we did order them with uni) a bit later. 5 of them. For the six of us. It is now an hour into our meal. We have already begun texting sweet republic, begging them to stay open. Or atleast bring us ice cream, because were hungry, damnit. \\n\\nA process is established. Every 30 minutes or so a single dish of sushi or something arrives. We each eat one piece, compliment it, and then spend 30 minutes digesting. Halfway through a waiter or waitress will appear to let us know something we ordered is out. But they have \\\"everything else\\\".  We have stopped being quiet and are now openly hysterical. And hungry.  At some point management approaches and we instruct him to turn around and go away until he has good news. \\n\\nThey are out of our sake, but are happy to suggest another that is 3x the price. One of the diners suggests they comp it. He laughs. After three hours, all still a wee bit hungry, but 80$ poorer and without a glimmer of hope for sweet republic, we depart. \\n\\nWe've been comped nothing. Not an \\\"I'm sorry\\\". Not a \\\"hey, have some edamame on us\\\". Not a \\\"we've knocked 10% off the bill\\\".  We are a bit hysterical over the comical situation, but also not a little pissed. A 400$ bill for a bunch of regulars with a reservation?  \\n\\nSigh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 star</td>\n",
       "      <td>My husband and I ordered a couple of recliners from this store in November.  We paid 1/2 up front and received a call from the owner a couple weeks later saying that one of our recliners was in but that the other one was sent to the wrong warehouse and wouldn't arrive until January.  My husband did pick up the recliner that was in but we're pretty sure we were given the floor model because the chair looks worn and also has a pretty bad squeak. The owner did deliver the 2nd recliner to our house when it arrived in January, but we weren't offered any kind of reimbursement for having to wait 2 months.  My husband did call the owner to let him know about the squeaky recliner and was told that he'd have to bring in the recliner so the manufacturer could provide us with a new one.  We didn't want to go through the hassle because we figured we'd have to wait another 2 months for a new recliner.  The owner was nice but I'm not sure that I'd ever buy furniture from this store again.  Most people do not want 2 months for their furniture to arrive and I felt the owner could have done more to compensate us for our troubles.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Great customer service and The staff is really nice. They do need more nail colors in stock. Overall great place.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>JUST OK. Sometimes its hard to explain exactly why something works for you and why other things don't. The atmosphere was pleasant and decorated as expected. The staff was friendly and service attentive. The food came quickly but we never felt rushed. As I'm writing this I'm struggling to use descriptive words ... summing up the overall experience, rather non-descript. If you're in the Mile End area and looking for a change from Quebecois cuisine then this will work well but I wouldn't recommend making a tremendous effort.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>I have been to Liberty a few times now. And I'm still not sure if I should give it a 3 or 4. The food wasn't memorable to me. But the biggest problem I have is that I can't keep up with the menu. Chef changes it all the time. I went 2 days ago and got a small plate of meatballs. When I looked up the menu online, the meatballs wasn't listed on there so I don't know the proper name to my dish. I guess I'm the type that if I really like something from a place. I will continue to go back and get the same thing. I had the pork skin and the boiled peanuts that I loved but is no longer on the dinner menu. \\nI also order the small plate of the mussels. By all means its NOT small. It was really good. But just couldn't finish it. \\nTheir customer service was good on someday and excellent on other days. My recent visit, she was good. Our table didn't have any salt or pepper so I was stuck w.a slightly bland meatballs. \\nOverall I will come back. Hopefully they will bring back the boiled peanuts. And I will find something I like. They do have a lot things on the menu I would like to try.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Went with the family on a Saturday night.  Casual enough for the kids.  I had lasagne.  Food was tasty, but all the portions were very small, especially the kids servings.  Not great for the money.  I prefer Nick's.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I am very picky with my Greek lemon soup. For instance, Daphneys has a good soup but I can make the same at home by adding a few things to canned soup. Now Athena has some amazing lemonsoup!  I got it one day on my way home from work and I was so glad I did. It was fresh and had the perfect flavor. Their pitas will make you drool. I still ha e to try more on their menu so this was definitely a review on the soup :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>We love sushi and we have been to some of the best sushi places in most of the major cities in the US and Canada. We are always looking for good sushi at a good price. So after getting a discounted gift certificate from restaurant.com we decided to try Cafe Wasabi. It's located in a strip mall but is nicely appointed inside. We spent a good while looking at the countless rolls available which were all pictured across the counter. This is perfect for those people who like to see what they are about to eat. We ordered a several different rolls and nigiri to sample it all. \\nWe spent about $70 for a two adults and 2 small kids. Good thing we had that gift certificate. Overall, the sushi was very good and the service was good. Again they have a great selection to choose from and I am looking forward to sampling more of the options.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('label', ClassLabel(names=['1 star', '2 star', '3 stars', '4 stars', '5 stars'], id=None)), ('text', Value(dtype='string', id=None))])\n",
      "<class 'datasets.features.features.ClassLabel'>\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![](./img/流程.png)\n",
    "\n",
    "- 使用 Tokenizer 编解码文本"
   ],
   "id": "13b93fe79fcb0eaa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:16:53.559608Z",
     "start_time": "2025-02-03T06:16:49.338765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gpt2 = pipeline(task=\"text-generation\",model=\"gpt2\", num_return_sequences=3)\n",
    "\n",
    "prompt = \"中国的首都\"\n",
    "\n",
    "gpt2(prompt)\n",
    "\n",
    "\n"
   ],
   "id": "c66c7d7ec354f7cc",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '中国的首都人黄中国的首无方法左司屜州十低其中国只麻去'},\n",
       " {'generated_text': '中国的首都至已。\\n\\n\"What are you going to do?\" He yelled.\\n\\nI\\'m going to be his teacher on the third grade!\\n\\n\"No… you\\'re not'},\n",
       " {'generated_text': '中国的首都应.\\n\\n灵宸望但仲是辦藏吧。 是边话计一个�'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:20:57.560364Z",
     "start_time": "2025-02-03T06:20:50.701871Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "bert = pipeline(task=\"fill-mask\",model=\"bert-base-chinese\")\n",
    "\n",
    "bert(\"中国[MASK]是个好地方\",top_k=3)\n",
    "\n",
    "\n",
    "bert(\"中国有个好地方[MASK][MASK]\",top_k=3)\n"
   ],
   "id": "fb081aac47baa674",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'score': 0.6023510098457336,\n",
       "   'token': 1408,\n",
       "   'token_str': '吗',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 吗 [MASK] [SEP]'},\n",
       "  {'score': 0.07599684596061707,\n",
       "   'token': 1416,\n",
       "   'token_str': '吧',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 吧 [MASK] [SEP]'},\n",
       "  {'score': 0.04747002199292183,\n",
       "   'token': 1557,\n",
       "   'token_str': '啊',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 啊 [MASK] [SEP]'}],\n",
       " [{'score': 0.6552453637123108,\n",
       "   'token': 8043,\n",
       "   'token_str': '？',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 [MASK] ？ [SEP]'},\n",
       "  {'score': 0.21130380034446716,\n",
       "   'token': 8013,\n",
       "   'token_str': '！',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 [MASK] ！ [SEP]'},\n",
       "  {'score': 0.13201002776622772,\n",
       "   'token': 511,\n",
       "   'token_str': '。',\n",
       "   'sequence': '[CLS] 中 国 有 个 好 地 方 [MASK] 。 [SEP]'}]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "“[CLS]”和“[SEP]”是在自然语言处理，特别是基于Transformer架构的预训练语言模型（如BERT）中使用的特殊标记。\n",
    "\n",
    "1. **[CLS]（Classification Token）**\n",
    "   - **含义**：它是“分类标记”。在BERT等模型中，当进行文本分类等任务时，会将输入文本的第一个词替换为[CLS]标记。模型最终对这个[CLS]标记对应的输出向量进行处理，以得到整个文本的特征表示，用于后续的分类决策。例如，在判断一段新闻文本属于政治、经济还是娱乐类别时，[CLS]标记经过模型处理后的向量就综合了整段新闻的关键信息，基于此向量进行分类。\n",
    "2. **[SEP]（Separator Token）**\n",
    "   - **含义**：表示“分隔标记”。在处理多个文本片段时，比如在问答任务中，问题和答案需要分开表示，就会用[SEP]来分隔不同的文本片段。例如，在BERT用于问答任务时，输入格式可能是[CLS]问题文本[SEP]答案文本[SEP]，这样模型可以通过[SEP]标记区分不同部分的文本，更好地理解文本结构和语义关系 。\n"
   ],
   "id": "ee6f20840ff2c0ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- Tokenizer 分词器",
   "id": "93bf8b354caaad09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:26:46.551342Z",
     "start_time": "2025-02-03T06:26:44.862902Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-chinese\")\n"
   ],
   "id": "3c5cf46a87f401",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 使用 BERT Tokenizer 编码文本\n",
    "\n",
    "编码 (Encoding) 过程包含两个步骤：\n",
    "\n",
    "- 分词：使用分词器按某种策略将文本切分为 tokens；\n",
    "- 映射：将 tokens 转化为对应的 token IDs。"
   ],
   "id": "f2ccc7130205c125"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:31:15.361774Z",
     "start_time": "2025-02-03T06:31:15.358358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = tokenizer.tokenize(\"中国的首都是北京\")\n",
    "print(tokens)\n",
    "\n",
    "\n",
    "tokenids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokenids)\n",
    "\n",
    "tokensencode = tokenizer.encode(\"中国的首都是北京\")   ## 多了一个 101,102\n",
    "print(tokensencode)\n",
    "\n",
    "decode =tokenizer.decode(tokenids)\n",
    "print(decode)\n",
    "\n"
   ],
   "id": "484cb7482de7871b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['中', '国', '的', '首', '都', '是', '北', '京']\n",
      "[704, 1744, 4638, 7674, 6963, 3221, 1266, 776]\n",
      "[101, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102]\n",
      "中 国 的 首 都 是 北 京\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 分词的文档  tokenizer.json 记录了所有分词对应的 tokenid",
   "id": "3828634f25e368c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T06:54:46.293372Z",
     "start_time": "2025-02-03T06:54:46.290165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embedding =  tokenizer([\"中国的首都是北京\",\"北京哈哈哈\",\"你好\"])\n",
    "print(embedding)"
   ],
   "id": "cb1d96973da65b7d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 704, 1744, 4638, 7674, 6963, 3221, 1266, 776, 102], [101, 1266, 776, 1506, 1506, 1506, 102], [101, 872, 1962, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

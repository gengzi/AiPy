{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 模型微调\n",
    "冻结某些层，通过特定数据，调整模型的某些层。\n",
    "需要数据集，对数据集进行预处理。 常见的方法有 截断和填充\n",
    "对于自然语言，需要通过分词器（token）将自然语言分割，并进行embedding，转为向量（固定长度） 每个单词，通过分词器就会映射到 词表对应的一个 tokenid 上\n",
    "进行模型的微调\n",
    "对微调后的模型进行 基准测试\n",
    "微调后模型的保存\n",
    "\n",
    "\n"
   ],
   "id": "6b44d6bcdbb1931"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- 数据处理\n",
    "- 加载数据\n",
    "- 数据预处理 - 截断和填充\n",
    "\n"
   ],
   "id": "370125acfd605005"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T10:31:23.171887Z",
     "start_time": "2025-02-05T10:30:49.503767Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer,pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(type(tokenizer))\n",
    "\n",
    "dataset =  load_dataset(\"yelp_review_full\")\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "dataset[\"train\"][111]\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True,padding=\"max_length\")\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function,batched=True)\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "print(small_train_dataset)\n",
    "\n"
   ],
   "id": "d4cc712c04e72b46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 650000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d25b144894af630c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T07:14:35.293952Z",
     "start_time": "2025-02-05T07:11:15.225713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=5)\n",
    "\n",
    "trainArgs = TrainingArguments(output_dir= \"/mnt/e/new_volume/models\",\n",
    "                                 per_device_train_batch_size=16,   ## 训练的批次大小，越大的批次，加载的数据越多\n",
    "                                  num_train_epochs=5, ## 整个数据集的遍历次数\n",
    "                                  logging_steps=100) ## 每隔一百次记录一次训练日志\n",
    "\n",
    "trainer = Trainer(model=model,args=trainArgs,  ## 模型参数\n",
    "                  train_dataset=small_train_dataset,   ## 训练集数据\n",
    "                    eval_dataset=small_eval_dataset)   ## 验证集数据\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "## 验证\n",
    "\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))\n",
    "trainer.evaluate(small_test_dataset)\n"
   ],
   "id": "34fd441eba0f63a6",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 03:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.645100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.335100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:01]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2619549036026,\n",
       " 'eval_runtime': 1.4347,\n",
       " 'eval_samples_per_second': 69.7,\n",
       " 'eval_steps_per_second': 9.061,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-03T12:02:03.738761Z",
     "start_time": "2025-02-03T12:02:03.622939Z"
    }
   },
   "cell_type": "code",
   "source": "! nvidia-smi",
   "id": "a30edd210ed50288",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "/bin/bash: line 1: nvidia-smi: command not found\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "54ec93aa05ea31e5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 问答任务的训练\n",
   "id": "a8863835df7e5939"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T07:25:43.593089Z",
     "start_time": "2025-02-05T07:25:35.376812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 加载数据集\n",
    "from datasets import load_dataset\n",
    "\n",
    "squad_data = load_dataset(\"squad\")\n",
    "\n",
    "## 每个数据集都有自己的格式和字段 ，还有训练集和测试集\n",
    "\n",
    "\n",
    "print(squad_data)\n"
   ],
   "id": "a3c411235336d9f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 87599\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 10570\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T07:34:57.920469Z",
     "start_time": "2025-02-05T07:34:57.202778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 预处理数据\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "token = tokenizer(\"北京在哪里？\",\"北京在中国\")\n",
    "\n",
    "print(token)\n"
   ],
   "id": "d8a2df6aada21290",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "{'input_ids': [101, 1781, 1755, 100, 100, 1962, 1994, 102, 1781, 1755, 100, 1746, 1799, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "34d7018ea96f3283"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

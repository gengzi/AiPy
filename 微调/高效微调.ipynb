{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PEFT 高效微调\n",
    "- 加载模型\n",
    "- 使用peft model （lora配置）添加适配器（adapter）\n",
    "- 训练数据准备\n",
    "- 模型训练\n",
    "- 模型保存\n",
    "- 模型推理\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "eb7677222515cb38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:12:48.046902Z",
     "start_time": "2025-02-13T13:12:48.044291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ],
   "id": "4c578c7c95e2cbf3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:51:56.114957Z",
     "start_time": "2025-02-13T13:51:52.292175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 准备数据\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Abirate/english_quotes\")\n",
    "\n",
    "print(dataset[\"train\"][100])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "ec704f2a44de5814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quote': '“I\\'m in love with you,\" he said quietly.\"Augustus,\" I said.\"I am,\" he said. He was staring at me, and I could see the corners of his eyes crinkling. \"I\\'m in love with you, and I\\'m not in the business of denying myself the simple pleasure of saying true things. I\\'m in love with you, and I know that love is just a shout into the void, and that oblivion is inevitable, and that we\\'re all doomed and that there will come a day when all our labor has been returned to dust, and I know the sun will swallow the only earth we\\'ll ever have, and I am in love with you.”', 'author': 'John Green,', 'tags': ['doomed', 'inevitable', 'love', 'oblivion', 'pleasure', 'simple']}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:52:05.718920Z",
     "start_time": "2025-02-13T13:51:58.330264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_id = f\"/mnt/e/models/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n"
   ],
   "id": "ad682cefe145784a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:44:37.525260Z",
     "start_time": "2025-02-13T13:44:37.420999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 为 8 位量化训练准备模型\n",
    "# from peft import prepare_model_for_int8_training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "bfe8beee52263160",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:52:31.660524Z",
     "start_time": "2025-02-13T13:52:31.607608Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## from peft import prepare_model_for_int8_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,  # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=32,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影\n",
    "    # target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc_in\", \"fc_out\"],\n",
    "    target_modules = [\"q_proj\"],\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型，这里设置为因果(自回归）语言模型\n",
    ")\n",
    "\n",
    "model = get_peft_model(model,config)\n",
    "\n",
    "# 打印出模型中可训练的参数\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# model = prepare_model_for_int8_training(model)"
   ],
   "id": "70fdb70408726dad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 344,064 || all params: 494,376,832 || trainable%: 0.0696\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:52:33.500947Z",
     "start_time": "2025-02-13T13:52:33.449778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"quote\"], padding=\"max_length\", truncation=True,max_length=512)\n",
    "\n",
    "# train_dataset = dataset[\"train\"]\n",
    "\n",
    "tokenizer_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "99fddb5db078ac81",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:52:35.946635Z",
     "start_time": "2025-02-13T13:52:35.939088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input = tokenizer(dataset[\"train\"][100][\"quote\"])\n",
    "print(input)"
   ],
   "id": "569ebaa3fd3a14d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [10168, 2776, 304, 2948, 448, 498, 1335, 566, 1053, 29566, 1189, 31459, 355, 1335, 358, 1053, 1189, 40, 1079, 1335, 566, 1053, 13, 1260, 572, 36774, 518, 752, 11, 323, 358, 1410, 1490, 279, 23462, 315, 806, 6414, 1560, 766, 2718, 13, 330, 40, 2776, 304, 2948, 448, 498, 11, 323, 358, 2776, 537, 304, 279, 2562, 315, 40466, 7037, 279, 4285, 16656, 315, 5488, 830, 2513, 13, 358, 2776, 304, 2948, 448, 498, 11, 323, 358, 1414, 429, 2948, 374, 1101, 264, 41123, 1119, 279, 737, 11, 323, 429, 66005, 290, 374, 30252, 11, 323, 429, 582, 2299, 678, 57637, 323, 429, 1052, 686, 2525, 264, 1899, 979, 678, 1039, 9327, 702, 1012, 5927, 311, 15797, 11, 323, 358, 1414, 279, 7015, 686, 41176, 279, 1172, 9393, 582, 3278, 3512, 614, 11, 323, 358, 1079, 304, 2948, 448, 498, 1987], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T13:56:59.935566Z",
     "start_time": "2025-02-13T13:56:59.933115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# 数据收集器，用于处理语言模型的数据，这里设置为不使用掩码语言模型(MLM)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ],
   "id": "bbf785c300ce7d21",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:14:15.653684Z",
     "start_time": "2025-02-13T13:57:01.097963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## 微调模型\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "trainingArgs = TrainingArguments(\n",
    "        output_dir=f\"/mnt/e/models/peft/qw-peft0.5\",  # 指定模型输出和保存的目录\n",
    "        per_device_train_batch_size=4,  # 每个设备上的训练批量大小\n",
    "        learning_rate=2e-4,  # 学习率\n",
    "        fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "        logging_steps=20,  # 指定日志记录的步长，用于跟踪训练进度\n",
    "        max_steps=100, # 最大训练步长\n",
    "        # num_train_epochs=1  # 训练的总轮数\n",
    "        gradient_accumulation_steps=4  # 梯度累积步数\n",
    ")\n",
    "\n",
    "train = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenizer_dataset[\"train\"],\n",
    "    args=trainingArgs,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "train.train()\n",
    "\n",
    "\n",
    "model.save_pretrained(\"/mnt/e/models/peft/qw-peft0.5\")\n",
    "tokenizer.save_pretrained(\"/mnt/e/models/peft/qw-peft0.5\")\n",
    "\n",
    "\n"
   ],
   "id": "b8593cf88cf4f7c9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.203200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.116900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.186300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('/mnt/e/models/peft/qw-peft0.5/tokenizer_config.json',\n",
       " '/mnt/e/models/peft/qw-peft0.5/special_tokens_map.json',\n",
       " '/mnt/e/models/peft/qw-peft0.5/vocab.json',\n",
       " '/mnt/e/models/peft/qw-peft0.5/merges.txt',\n",
       " '/mnt/e/models/peft/qw-peft0.5/added_tokens.json',\n",
       " '/mnt/e/models/peft/qw-peft0.5/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-13T14:19:29.465993Z",
     "start_time": "2025-02-13T14:19:25.968803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "newmodel = train.model\n",
    "\n",
    "text = \"No matter how careful you are\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(0)\n",
    "\n",
    "out = newmodel.generate(**inputs, max_new_tokens=48)\n",
    "print(tokenizer.decode(out[0], skip_special_tokens=True))"
   ],
   "id": "4a7be887e31e027c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matter how careful you are, there is always a chance that you will make a mistake. In fact, it's not uncommon for most people to make mistakes at some point in their lives. The key to avoiding these mistakes is to be prepared and take steps to avoid\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

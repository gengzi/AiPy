{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-14T11:54:48.795793Z",
     "start_time": "2025-02-14T11:54:48.794142Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 语音任务微调\n",
    "- 下载数据集 Common Voice\n",
    "- 调整数据集\n",
    "- 下载模型\n",
    "- 使用 peft 技术对模型 微调 （lora）\n",
    "- 训练\n",
    "-"
   ],
   "id": "6374fb97844993f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "6e504f33409e7c79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T11:36:32.576360Z",
     "start_time": "2025-02-15T11:36:28.500225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset,DatasetDict,Audio\n",
    "\n",
    "from 微调.高效微调 import tokenizer_dataset\n",
    "\n",
    "dataset_name = \"mozilla-foundation/common_voice_11_0\"\n",
    "language_abbr = \"zh-CN\"\n",
    "dataset_dict = DatasetDict()\n",
    "\n",
    "dataset_dict[\"train\"]  = load_dataset(dataset_name,language_abbr,split=\"train\",trust_remote_code=True)\n",
    "dataset_dict[\"validation\"] = load_dataset(dataset_name,language_abbr,split=\"validation\",trust_remote_code=True)\n",
    "\n",
    "\n",
    "print(dataset_dict[\"train\"][10])\n",
    "\n",
    "## 移除多余的列\n",
    "dataset_dict = dataset_dict.remove_columns([\n",
    "    \"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"\n",
    "])\n",
    "\n",
    "## 降采样\n",
    "\n",
    "dataset_dict = dataset_dict.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "\n",
    "\n",
    "print(dataset_dict[\"train\"][10])\n",
    "\n",
    "\n"
   ],
   "id": "10877a8617b2d380",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'client_id': '95368aab163e0387e4fd4991b4f2d8ccfbd4364bf656c860230501fd27dcedf087773e4695a6cf5de9c4f1d406d582283190d065cdfa36b0e2b060cffaca977e', 'path': '/home/gengzi/.cache/huggingface/datasets/downloads/extracted/4ac968b0af73b59a30074b20435d7c35bd883441b69599a338fd8f04b5d01129/zh-CN_train_0/common_voice_zh-CN_33211831.mp3', 'audio': {'path': '/home/gengzi/.cache/huggingface/datasets/downloads/extracted/4ac968b0af73b59a30074b20435d7c35bd883441b69599a338fd8f04b5d01129/zh-CN_train_0/common_voice_zh-CN_33211831.mp3', 'array': array([2.84217094e-14, 2.98427949e-13, 3.69482223e-13, ...,\n",
      "       1.02614513e-05, 9.40982409e-06, 4.53803295e-06]), 'sampling_rate': 48000}, 'sentence': '参与本电影制作的工作人员们担任了牵引之后日本动画界的角色。', 'up_votes': 2, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'zh-CN', 'segment': ''}\n",
      "{'audio': {'path': '/home/gengzi/.cache/huggingface/datasets/downloads/extracted/4ac968b0af73b59a30074b20435d7c35bd883441b69599a338fd8f04b5d01129/zh-CN_train_0/common_voice_zh-CN_33211831.mp3', 'array': array([-5.45696821e-12, -1.45519152e-11, -3.63797881e-12, ...,\n",
      "       -9.23988409e-07, -1.46567618e-06,  9.17912985e-06]), 'sampling_rate': 16000}, 'sentence': '参与本电影制作的工作人员们担任了牵引之后日本动画界的角色。'}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "46e046f59208223b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T11:33:43.985384Z",
     "start_time": "2025-02-15T11:33:31.742837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModel\n",
    "\n",
    "model_name_or_path = \"openai/whisper-large-v2\"\n",
    "# 从预训练模型加载特征提取器\n",
    "featureExtractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,language=language_abbr)\n",
    "\n",
    "# 从预训练模型加载处理器，处理器通常结合了特征提取器和分词器，为特定任务提供一站式的数据预处理\n",
    "processor = AutoProcessor.from_pretrained(model_name_or_path,language=language_abbr)\n",
    "\n",
    "\n"
   ],
   "id": "1de2278aaf608d11",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## 数据预处理\n",
   "id": "2c18797aa567c8df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 数据预处理",
   "id": "a66c76f32604f031"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T11:48:14.505391Z",
     "start_time": "2025-02-15T11:48:14.502273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def prepare_dataset(examples):\n",
    "    audio = examples[\"audio\"]\n",
    "    examples[\"input_features\"] = featureExtractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    examples[\"labels\"] = tokenizer(examples[\"sentence\"]).input_ids\n",
    "    return examples\n",
    "\n"
   ],
   "id": "766da44ecbda5efc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "13b39a24e47ed660"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 数据抽样",
   "id": "aaf35b40ecfd5a1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T11:46:58.077357Z",
     "start_time": "2025-02-15T11:46:58.069047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "small_dataset = DatasetDict()\n",
    "small_dataset[\"train\"] =   dataset_dict[\"train\"].shuffle(seed=11).select(range(500))\n",
    "small_dataset[\"validation\"] = dataset_dict[\"validation\"].shuffle(seed=11).select(range(200))\n",
    "\n",
    "print(small_dataset)"
   ],
   "id": "82da70ab5402ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'sentence'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d1360ef98647a3b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T11:48:47.461670Z",
     "start_time": "2025-02-15T11:48:16.194852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer_dataset = small_dataset.map(prepare_dataset)\n",
    "\n",
    "print(tokenizer_dataset)"
   ],
   "id": "d4a351b97c59b0d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 500/500 [00:22<00:00, 22.57 examples/s] \n",
      "Map: 100%|██████████| 200/200 [00:09<00:00, 22.04 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['audio', 'sentence', 'input_features', 'labels'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e22f1257fa5887af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 将数据集中的内容进行，填充和截断\n",
   "id": "bb461a7951e549d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:09:43.180207Z",
     "start_time": "2025-02-15T12:09:43.176017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n"
   ],
   "id": "5969df6a2d48e80",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:09:44.457907Z",
     "start_time": "2025-02-15T12:09:44.455658Z"
    }
   },
   "cell_type": "code",
   "source": "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
   "id": "dca88a2a67f9cbe1",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:33:17.027476Z",
     "start_time": "2025-02-15T12:14:10.091328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_name_or_path,load_in_4bit =True,device_map=\"auto\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "7da298b30c24f307",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:38:19.419134Z",
     "start_time": "2025-02-15T12:38:19.403217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)"
   ],
   "id": "d8e48d845ae11124",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:35:35.420121Z",
     "start_time": "2025-02-15T12:35:35.417178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n"
   ],
   "id": "1326d86296c883d5",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 准备高效微调的参数",
   "id": "4494aa89b73f4bfc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:35:37.395189Z",
     "start_time": "2025-02-15T12:35:37.318620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig,get_peft_model\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "     r=4,  # LoRA的秩，影响LoRA矩阵的大小\n",
    "    lora_alpha=64,  # LoRA适应的比例因子\n",
    "    # 指定将LoRA应用到的模型模块，通常是attention和全连接层的投影。\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,  # 在LoRA模块中使用的dropout率\n",
    "    bias=\"none\",  # 设置bias的使用方式，这里没有使用bias\n",
    ")\n"
   ],
   "id": "bdb62522b3942f7c",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:35:39.216961Z",
     "start_time": "2025-02-15T12:35:38.994513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_model =get_peft_model(model,config)\n",
    "\n",
    "# 打印 LoRA 微调训练的模型参数\n",
    "peft_model.print_trainable_parameters()"
   ],
   "id": "234f646f67ebc2d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,966,080 || all params: 1,545,271,040 || trainable%: 0.1272\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "48b71d08f0a92436"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 添加训练参数",
   "id": "f4d164d75e4bf0a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:50:23.632965Z",
     "start_time": "2025-02-15T12:50:23.628748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "model_dir = \"/mnt/e/models/peft/whisper\"\n",
    "\n",
    "trainingArguments = Seq2SeqTrainingArguments(\n",
    "    output_dir=model_dir,  # 指定模型输出和保存的目录\n",
    "    per_device_train_batch_size=20,  # 每个设备上的训练批量大小\n",
    "    learning_rate=1e-3,  # 学习率\n",
    "    num_train_epochs=10,  # 训练的总轮数\n",
    "    evaluation_strategy=\"epoch\",  # 设置评估策略，这里是在每个epoch结束时进行评估\n",
    "    # warmup_steps=50,  # 在训练初期增加学习率的步数，有助于稳定训练\n",
    "    # fp16=True,  # 启用混合精度训练，可以提高训练速度，同时减少内存使用\n",
    "    per_device_eval_batch_size=10,  # 每个设备上的评估批量大小\n",
    "    generation_max_length=128,  # 生成任务的最大长度\n",
    "    logging_steps=10,  # 指定日志记录的步骤，用于跟踪训练进度\n",
    "    remove_unused_columns=False,  # 是否删除不使用的列，以减少数据处理开销\n",
    "    label_names=[\"labels\"],  # 指定标签列的名称，用于训练过程中\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=25,\n",
    ")\n",
    "\n",
    "\n"
   ],
   "id": "b243b27c544d9f3f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:50:25.125270Z",
     "start_time": "2025-02-15T12:50:25.116560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "\n",
    "train= Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=trainingArguments,\n",
    "    train_dataset=tokenizer_dataset[\"train\"],\n",
    "    eval_dataset=tokenizer_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "\n",
    ")\n",
    "peft_model.config.use_cache = False"
   ],
   "id": "3c43ef977d44ab86",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2089/3386571359.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  train= Seq2SeqTrainer(\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:51:04.110383Z",
     "start_time": "2025-02-15T12:50:26.294773Z"
    }
   },
   "cell_type": "code",
   "source": "train.train()",
   "id": "a2583025102dad",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/250 00:18 < 37:49, 0.11 it/s, Epoch 0.12/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m train\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m~/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/trainer.py:2164\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2162\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2163\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2164\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2165\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2166\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2167\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2168\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2169\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/trainer.py:2529\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2523\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[1;32m   2524\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs, num_items_in_batch)\n\u001B[1;32m   2526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2527\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2528\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[0;32m-> 2529\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2530\u001B[0m ):\n\u001B[1;32m   2531\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2532\u001B[0m     tr_loss \u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m+\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n\u001B[1;32m   2533\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "69ee26faed640fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T12:47:01.668545Z",
     "start_time": "2025-02-15T12:46:59.680549Z"
    }
   },
   "cell_type": "code",
   "source": "train.save_model(model_dir)",
   "id": "467fea14bf5d9688",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 测试",
   "id": "d7440512f191f3d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:08:02.386400Z",
     "start_time": "2025-02-15T13:07:17.543120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq,AutoTokenizer,AutoProcessor\n",
    "from peft import LoraConfig,get_peft_model,PeftConfig,PeftModel\n",
    "model_dir1 = \"/mnt/e/models/peft/whisper\"\n",
    "peft_config = PeftConfig.from_pretrained(model_dir1)\n",
    "\n",
    "basemodel = AutoModelForSpeechSeq2Seq.from_pretrained(peft_config.base_model_name_or_path,load_in_4bit =True,device_map=\"auto\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(basemodel,model_dir1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_config.base_model_name_or_path, language=language_abbr)\n",
    "processor = AutoProcessor.from_pretrained(peft_config.base_model_name_or_path, language=language_abbr)\n",
    "feature_extractor = processor.feature_extractor\n",
    "\n",
    "text_audio = \"/mnt/e/ruanjian/audio.wav\"\n",
    "\n",
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "\n",
    "pipeline = AutomaticSpeechRecognitionPipeline(model=peft_model, tokenizer=tokenizer, feature_extractor=feature_extractor)\n",
    "\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"chinese\")\n",
    "\n",
    "\n"
   ],
   "id": "7e07beb6a83d142",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:11:38.009774Z",
     "start_time": "2025-02-15T13:11:36.614266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "text_audio1 = \"/mnt/e/ruanjian/audio.wav\"\n",
    "with torch.cuda.amp.autocast():\n",
    "    text = pipeline(text_audio1, max_new_tokens=255)[\"text\"]"
   ],
   "id": "9c5e3353091d2d72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2089/3896084746.py:3: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/pipelines/automatic_speech_recognition.py:312: FutureWarning: `max_new_tokens` is deprecated and will be removed in version 4.49 of Transformers. To remove this warning, pass `max_new_tokens` as a key inside `generate_kwargs` instead.\n",
      "  warnings.warn(\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/transformers/models/whisper/generation_whisper.py:512: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "/home/gengzi/anaconda3/envs/env-vllm/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:446: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T13:11:43.659311Z",
     "start_time": "2025-02-15T13:11:43.657085Z"
    }
   },
   "cell_type": "code",
   "source": "print(text)",
   "id": "a800bcfc2821b433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好大計畫!那是否答覆阿道夫?答覆大師!\n"
     ]
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# ollama 部署本地大模型\n",
    " - ollama 官方地址：https://ollama.com/  https://github.com/ollama/ollama\n",
    "ollama 使用gguf 格式的大模型文件，使用  Modelfile  来配置大模型的一些属性（比如使用那个模型，系统提示词，上下文参数大小等）\n",
    "Modelfile 相关配置：https://github.com/ollama/ollama/blob/main/docs/modelfile.md\n",
    "\n",
    "本示例，部署deepseek-r1:1.5b-qwen-distill-q4_K_M 量化后的模型\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "f3a30b20fcd3104a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## ollama 安装\n",
    "针对不同的平台，ollama都提供了安装包，进行安装即可。\n",
    "\n",
    "## 模型下载与安装\n",
    "- 直接使用 https://ollama.com/library\n",
    "找到deepseek-r1 模型，选择一个模型，复制运行命令\n",
    "\n",
    "```shell\n",
    "ollama run deepseek-r1:1.5b-qwen-distill-q8_0\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- 将正常的模型文件转化为gguf 格式的文件\n",
    "\n",
    "- 直接下载gguf 格式的大模型文件\n",
    "\n",
    "### 手动配置 modelfile 文件\n",
    "\n",
    "### 模式使用\n",
    "\n",
    "- 客户端：直接在命令行中访问大模型\n",
    "\n",
    "- 服务端： 提供api形式访问大模型\n",
    "\n",
    "\n",
    "## 一些特殊配置\n",
    "- 配置下载模型文件的默认地址\n",
    "ollama 在win下，模型下载model的位置是： C:\\Users\\<你的用户名>\\.ollama\\models\n",
    "\n",
    "如果你的c盘空间不足，可以调整到其他目录：\n",
    "\n",
    "\n",
    "## 可视化界面Openweb ui\n",
    "https://openwebui.com/\n",
    "https://github.com/open-webui/open-webui\n"
   ],
   "id": "7f7153e07f8ca8b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fa777aca45cc3c58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f381177903c4d22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "[toc]\n",
    "\n",
    "\n",
    "\n",
    "# ollama 本地部署deepseek-r1-1.5b模型\n",
    "\n",
    "## 概述\n",
    "\n",
    "ollama 是一个强大的工具，用于本地部署大语言模型。其官方地址为：https://ollama.com/ 和 https://github.com/ollama/ollama 。ollama 使用 gguf 格式的大模型文件，这种格式具有诸多优势，比如存储效率高、加载速度快等。同时，ollama 使用 Modelfile 来配置大模型的一些关键属性，例如使用哪个具体的模型、系统提示词以及上下文参数大小等。关于 Modelfile 的详细配置信息，可以参考：https://github.com/ollama/ollama/blob/main/docs/modelfile.md 。\n",
    "\n",
    "本示例将重点介绍如何部署 `deepseek-r1:1.5b-qwen-distill-q4_K_M` 量化后的模型。\n",
    "\n",
    "## ollama 安装\n",
    "\n",
    "ollama 针对不同的操作系统平台，都提供了相应的安装包，用户可以根据自己的系统环境选择合适的安装包进行安装。以下是一些常见平台的安装说明：\n",
    "\n",
    "### Windows 平台\n",
    "\n",
    "1. 访问 ollama 官方网站的下载页面。\n",
    "2. 下载适用于 Windows 系统的安装包（通常为.exe 文件）。\n",
    "3. 双击安装包，按照安装向导的提示完成安装过程。\n",
    "\n",
    "### Linux 平台\n",
    "\n",
    "- Debian/Ubuntu 系统：\n",
    "  - 可以使用 apt 包管理工具进行安装。首先，添加 ollama 的软件源到系统的源列表中。然后，执行 `sudo apt update` 更新软件包列表，最后执行 `sudo apt install ollama` 进行安装。\n",
    "- CentOS/RHEL 系统：\n",
    "  - 可以使用 yum 包管理工具。先添加 ollama 的软件源，再执行 `sudo yum update` 更新软件包列表，最后执行 `sudo yum install ollama` 完成安装。\n",
    "\n",
    "### macOS 平台\n",
    "\n",
    "- 可以使用 Homebrew 包管理工具进行安装。在终端中执行 `brew install ollama` 即可。\n",
    "\n",
    "## 模型下载与安装\n",
    "\n",
    "在 ollama 中，有多种方式可以获取和安装模型，以下是详细介绍：\n",
    "\n",
    "### 通过 ollama 官方库下载\n",
    "\n",
    "- 直接访问 https://ollama.com/library 。\n",
    "- 在页面中找到 `deepseek-r1` 模型，根据自己的需求选择一个具体的模型版本。\n",
    "- 复制该模型的运行命令，例如：\n",
    "\n",
    "```shell\n",
    "ollama run deepseek-r1:1.5b-qwen-distill-q4_K_M\n",
    "```\n",
    "\n",
    "- 执行上述命令后，ollama 会自动下载该模型相关的文件信息。\n",
    "\n",
    "### 直接下载 gguf 格式的大模型文件\n",
    "\n",
    "#### 第一步：从 Hugging Face 下载模型文件\n",
    "\n",
    "![](./image-20250221153625443.png)\n",
    "\n",
    "- 访问 Hugging Face 平台，例如进入 https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B 页面，这里可以看到有关量化或者微调后的模型。\n",
    "- 找到需要的 gguf 格式模型文件，例如 https://huggingface.co/bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-1.5B-Q6_K.gguf 。\n",
    "- 点击下载链接，模型文件将被下载到本地计算机。\n",
    "\n",
    "#### 第二步：创建并配置 Modelfile 文件\n",
    "\n",
    "- 在与下载后的模型文件相同的目录中，创建一个名为 `Modelfile` 的文件（注意，该文件没有文件格式后缀）。\n",
    "- 使用文本编辑器打开 `Modelfile` 文件，输入以下内容：\n",
    "\n",
    "```yaml\n",
    "FROM ./DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf\n",
    "PARAMETER num_ctx 4096\n",
    "PARAMETER temperature 0.8\n",
    "SYSTEM \"\"\"\n",
    "你是人工智能助手，请解答我的问题\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "以下是对这段 `Modelfile` 中各个参数和语句的详细解释：\n",
    "\n",
    "- **`FROM ./DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf`**\n",
    "\n",
    "  - **功能**：在 Ollama 中，`FROM` 语句用于指定模型的来源，表明基于哪个基础模型来构建新的模型或者进行配置。\n",
    "  - **含义**：这里使用的是本地文件系统中相对路径下的 `DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.gguf` 文件作为基础模型。`.gguf` 是一种模型文件格式，常用于存储大语言模型的权重等数据。这意味着后续的配置和交互都会基于这个特定的模型来进行。\n",
    "\n",
    "- **`PARAMETER num_ctx 4096`**\n",
    "\n",
    "  - **功能**：`PARAMETER` 用于设置模型运行时的各种参数，`num_ctx` 代表上下文窗口大小。\n",
    "  - **含义**：上下文窗口大小决定了模型在生成文本时能够参考的前文的最大长度，单位是 token（标记，文本处理中的基本单位）。将 `num_ctx` 设置为 4096，意味着模型在生成下一个 token 时，最多可以考虑之前的 4096 个 token 作为上下文信息。更大的上下文窗口能让模型在处理长文本或者需要前后文关联的任务时表现更好，例如在续写长篇文章、理解复杂对话等场景中具有优势。\n",
    "\n",
    "- **`PARAMETER temperature 0.8`**\n",
    "\n",
    "  - **功能**：同样是使用 `PARAMETER` 来设置参数，`temperature` 是控制模型输出随机性的一个重要参数。\n",
    "\n",
    "  - 含义：\n",
    "\n",
    "    temperature的取值范围通常是大于 0 的实数。其值的大小会影响模型生成文本的随机性和创造性：\n",
    "\n",
    "    - 当 `temperature` 值较高（例如接近 1 或者更大）时，模型生成的文本会更具随机性和创造性，可能会给出更多新颖、多样化的回答，但有时可能会偏离逻辑或者连贯性。\n",
    "    - 当 `temperature` 值较低（例如接近 0）时，模型生成的文本会更加保守、连贯，更倾向于选择概率较高的 token，输出结果相对比较确定和稳定。这里设置为 0.8，说明希望模型在生成文本时既有一定的随机性和创造性，又能保持较好的连贯性。\n",
    "\n",
    "- **`SYSTEM \"你是人工智能助手，请解答我的问题\"`**\n",
    "\n",
    "  - **功能**：`SYSTEM` 语句用于设置系统提示信息，其作用是告知模型它应该扮演的角色和行为方式。\n",
    "  - **含义**：此系统提示要求模型将自己定位为一个人工智能助手，并且在与用户交互时，主要的任务是解答用户提出的问题。模型在生成回复时会参考这个系统提示，以符合设定的角色和任务要求。\n",
    "\n",
    "#### 第三步：关联 modelfile 和模型文件\n",
    "\n",
    "使用 ollama 创建模型，将 `modelfile` 和模型文件关联起来，执行以下命令：\n",
    "\n",
    "```shell\n",
    "ollama create deepseek-r1:1.5b-q2 -f Modelfile\n",
    "```\n",
    "\n",
    "#### 第四步：运行模型\n",
    "\n",
    "执行以下命令来运行模型：\n",
    "\n",
    "```shell\n",
    "ollama run deepseek-r1:1.5b-q2\n",
    "```\n",
    "\n",
    "### 将正常的模型文件转化为 gguf 格式的文件\n",
    "\n",
    "有时候，我们可能需要将正常的模型文件转化为 GGUF 格式的文件，以常见的基于 llama.cpp 框架转换为例，具体步骤如下：\n",
    "\n",
    "#### 1. 下载 llama.cpp\n",
    "\n",
    "从 [官方 GitHub](https://github.com/ggerganov/llama.cpp.git) 克隆 llama.cpp 项目代码，命令为：\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/ggerganov/llama.cpp.git\n",
    "```\n",
    "\n",
    "#### 2. 编译 llama.cpp\n",
    "\n",
    "- Windows 系统：\n",
    "  - 先安装 cmake 工具，从 [CMake 官网](https://cmake.org/download/) 下载并按照教程安装。\n",
    "  - 安装完成后进入 llama.cpp 目录，执行以下命令：\n",
    "\n",
    "```shell\n",
    "mkdir build\n",
    "cd build\n",
    "cmake..\n",
    "cmake --build. --config release\n",
    "```\n",
    "\n",
    "- Linux 和 macOS 系统：在 llama.cpp 目录下直接执行 `make` 命令即可：\n",
    "\n",
    "```shell\n",
    "make\n",
    "```\n",
    "\n",
    "#### 3. 安装必要的 Python 包\n",
    "\n",
    "进入 llama.cpp 目录，执行以下命令安装所需的 Python 依赖包：\n",
    "\n",
    "```shell\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### 4. 转换模型格式\n",
    "\n",
    "使用 `convert.py` 或 `convert-hf-to-gguf.py` 脚本进行转换。例如，转换 Llama3 模型为 GGUF 格式，可使用以下命令：\n",
    "\n",
    "```shell\n",
    "python3./convert.py /root/work/models/Llama3-Chinese-8B-Instruct/ --outtype f16 --vocab-type bpe --outfile./models/Llama3-FP16.gguf\n",
    "```\n",
    "\n",
    "#### 5. 量化 GGUF 模型\n",
    "\n",
    "若需要量化模型，可使用 `quantize` 工具。例如：\n",
    "\n",
    "```shell\n",
    "./llama.cpp/quantize Qwen2-7B-Instruct.gguf Qwen2-7B-Instruct-Q5_K_M.gguf Q5_K_M\n",
    "```\n",
    "\n",
    "该命令将 `Qwen2-7B-Instruct.gguf` 模型量化为 `Q5_K_M` 精度的 `Qwen2-7B-Instruct-Q5_K_M.gguf` 。\n",
    "\n",
    "再配置modelfile文件运行即可\n",
    "\n",
    "## 手动配置 modelfile 文件\n",
    "\n",
    "关于 modelfile 文件的手动配置，可以参考官方文档：https://github.com/ollama/ollama/blob/main/docs/modelfile.md 。在这个文件中，我们可以对模型来源、上下文大小、系统提示词等进行详细的配置。\n",
    "\n",
    "## 不同模式使用大模型\n",
    "\n",
    "### 客户端：直接在命令行中访问大模型\n",
    "\n",
    "在命令行中执行以下命令，即可直接访问大模型：\n",
    "\n",
    "```shell\n",
    "ollama run deepseek-r1:1.5b-qwen-distill-q4_K_M\n",
    "```\n",
    "\n",
    "执行命令后，会进入交互模式，你可以输入问题，模型会给出相应的回答，例如：\n",
    "\n",
    "```plaintext\n",
    "PS C:\\Users\\Administrator> ollama run deepseek-r1:1.5b-qwen-distill-q4_K_M\n",
    ">>> 你好\n",
    "<think>\n",
    "\n",
    "</think>\n",
    "\n",
    "你好！很高兴见到你，有什么我可以帮忙的吗？\n",
    "\n",
    ">>> Send a message (/? for help)\n",
    "```\n",
    "\n",
    "### 服务端：提供 API 形式访问大模型\n",
    "\n",
    "可以通过以下命令启动 ollama 服务：\n",
    "\n",
    "```shell\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "在启动服务时，可能会遇到端口占用的问题，例如：\n",
    "\n",
    "```plaintext\n",
    "PS C:\\Users\\Administrator> ollama serve\n",
    "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n",
    "```\n",
    "\n",
    "这表明 127.0.0.1:11434 端口已经被其他程序占用，需要先解决端口冲突问题。\n",
    "\n",
    "当服务正常启动后，可以使用以下命令通过 API 访问模型：\n",
    "\n",
    "```shell\n",
    "curl http://localhost:11434/api/generate -d '{\n",
    "    \"model\": \"deepseek-r1:1.5b-qwen-distill-q4_K_M\",\n",
    "    \"prompt\": \"你好吗?\"\n",
    "}'\n",
    "```\n",
    "\n",
    "关于 API 的详细文档，可以参考官方文档：https://github.com/ollama/ollama/blob/main/docs/api.md 。\n",
    "\n",
    "## 一些特殊配置\n",
    "\n",
    "### 配置下载模型文件的默认地址\n",
    "\n",
    "ollama 在 Windows 系统下，模型下载的默认位置是：`C:\\Users\\<你的用户名>\\.ollama\\models` 。如果你的 C 盘空间不足，可以将模型下载位置调整到其他目录，具体方法如下：\n",
    "\n",
    "#### 通过环境变量永久修改\n",
    "\n",
    "1. **设置系统环境变量**\n",
    "   - 右键点击 **此电脑**，选择 **属性**，在弹出的窗口中点击 **高级系统设置**，再点击 **环境变量**。\n",
    "   - 在系统变量区域点击新建：\n",
    "     - 变量名：`OLLAMA_MODELS`\n",
    "     - 变量值：新路径（如 `D:\\AI\\Models`）\n",
    "   - 确认保存所有对话框。\n",
    "2. **重启生效**\n",
    "   - 重启计算机使环境变量生效。\n",
    "3. **验证路径**\n",
    "   - 打开 PowerShell 执行以下命令：\n",
    "\n",
    "```powershell\n",
    "echo $env:OLLAMA_MODELS\n",
    "```\n",
    "\n",
    "如果显示的是 `D:\\AI\\Models` ，则说明环境变量设置成功。\n",
    "\n",
    "## 可视化界面 Openweb UI\n",
    "\n",
    "Openweb UI 是一个用于访问本地部署的大模型的可视化界面，其官方网站为：https://openwebui.com/ ，GitHub 仓库地址为：https://github.com/open-webui/open-webui 。\n",
    "\n",
    "通过 openwebui 访问本地部署的大模型非常简单，具体步骤如下：\n",
    "\n",
    "### 安装\n",
    "\n",
    "使用以下命令安装 openwebui：\n",
    "\n",
    "```shell\n",
    "pip install open-webui\n",
    "```\n",
    "\n",
    "### 启动\n",
    "\n",
    "安装完成后，使用以下命令启动 openwebui：\n",
    "\n",
    "```shell\n",
    "open-webui serve\n",
    "```\n",
    "\n",
    "### 访问\n",
    "\n",
    "启动后，访问 http://localhost:8080/ 即可进入 Openweb UI 界面。\n",
    "\n",
    "openwebui 通过 ollama serve 提供的 API 接口访问当前运行的大模型。在界面中，点击用户头像，进入管理员界面，然后点击 **设置 - 外部链接** 进行相关配置。\n",
    "\n",
    "![image-20250221152739225](./image-20250221152739225.png)\n",
    "\n",
    "![image-20250221152802225](./image-20250221152802225.png)\n",
    "\n",
    "配置好后，就可以跟当前运行的模型进行聊天了\n",
    "\n",
    "![image-20250221152927731](./image-20250221152927731.png)\n",
    "\n",
    "## 常见问题及解决方法\n",
    "\n",
    "### 端口冲突问题\n",
    "\n",
    "当启动 `ollama serve` 时，可能会遇到端口冲突的问题，提示信息如下：\n",
    "\n",
    "```plaintext\n",
    "Error: listen tcp 127.0.0.1:11434: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted.\n",
    "```\n",
    "\n",
    "**解决方法**：\n",
    "\n",
    "- 可以通过修改 `ollama serve` 的端口号来解决冲突。例如，使用 `ollama serve --host 0.0.0.0 --port 8081` 命令将端口号修改为 8081。\n",
    "- 也可以检查并关闭占用 11434 端口的其他程序。\n",
    "\n",
    "### 模型下载缓慢问题\n",
    "\n",
    "在下载模型时，可能会遇到下载速度缓慢的问题。\n",
    "\n",
    "**解决方法**：\n",
    "\n",
    "- 可以尝试更换网络环境，例如使用高速稳定的网络连接。\n",
    "- 也可以使用代理服务器，在命令行中设置相应的代理环境变量，例如：\n",
    "\n",
    "```shell\n",
    "export HTTP_PROXY=http://proxy.example.com:8080\n",
    "export HTTPS_PROXY=http://proxy.example.com:8080\n",
    "```\n",
    "\n",
    "## 总结\n",
    "\n",
    "通过以上步骤，你可以在本地成功部署 ollama 并使用 `deepseek-r1:1.5b-qwen-distill-q4_K_M` 模型。同时，你还可以根据自己的需求，对模型的配置、运行模式等进行灵活调整。在使用过程中，如果遇到问题，可以参考本文档中的常见问题及解决方法，或者查阅官方文档获取更多帮助。"
   ],
   "id": "6b61f97d6f883e86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "388937f131d2a49d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "53643e12fb52acc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![](./image-20250221153625443.png)",
   "id": "549c6ad1f1f64922"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
